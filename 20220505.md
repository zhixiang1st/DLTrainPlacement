# 20220505学习小结
## 权重参数的最优化
1. SGD：随机梯度下降法
$W\leftarrow W- \eta\frac{\partial L}{\partial W} $
> 缺点：如果函数非均向，搜索路径会非常低效
```python
#SGD的实现
class SGD:
    def __init__(self,lr) :
        self.lr = lr
    def update(self,params,grads):
        for key in params.keys():
            params[key] = params[key] - self.lr*grads[key]
```
2. Momentum
$v\leftarrow \alpha v-\eta\frac{\partial L}{\partial W}$
$W\leftarrow W+v $
其中，$\ \alpha v$承担物体不受力时减小的任务（$\ \alpha$通常取0.9等）$\ \eta $表示学习率，$\ v$表示沿梯度的速度
```python
#Momentum的实现
class Momentum:
    def __init__(self,lr,momentum = 0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None
    def update(self,params,grads):
        #!!!向self.v里传递初始参数矩阵
        if self.v is None:
            self.v = {}
            for key,val in params.items():
                self.v[key] = np.zeros_like(val)
        for key in params.keys():
            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]
            params[key] = params[key] + self.v[key]
```
3. AdaGrad
学习率$\ \eta $在神经网络的学习中起到很重要的作用。过小会导致花费过多时间，过大会导致学习发散。
> 有一种被称为“学习率衰减”(learning rate decay)的方法，即随着学习的进行，学习率逐渐减小，这在神经网络的学习中经常被引用。

$h\leftarrow h+\frac{\partial L}{\partial W}\odot \frac{\partial L}{\partial W}$
$W\leftarrow W-\eta\frac{1}{\sqrt{n}}\frac{\partial L}{\partial W}$
其中，$\ h$表存了以前所有梯度的平方和（$\ \odot$表示矩阵各元素相乘）在更新参数时，通过乘以$\ \frac{1}{\sqrt{n}}$来使参数变动大的学习率减小。
```python
#Adagrad的实现
class AdaGrad:
    def __init__(self,lr) :
        self.lr = lr
        self.h = None
    def update(self,params,grads):
        if self.h is None:
            self.h = {}
            for key,val in params.items():
                self.h[key] = np.zeros_like(val)
        for key in params.keys():
            self.h[key] =  self.h[key] + grads[key]**2
            params[key] = params[key] - self.lr*grads[key]/( np.sqrt(self.h[key]) + 1e-7)
```
直观测试：
```python
def f(x, y):
    return x**2 / 20.0 + y**2


def df(x, y):
    return x / 10.0, 2.0*y

init_pos = (-7.0, 2.0)
params = {}
params['x'], params['y'] = init_pos[0], init_pos[1]
grads = {}
grads['x'], grads['y'] = 0, 0
```
效果图：
![](https://s3.bmp.ovh/imgs/2022/05/05/341228d55a2f4517.png)
## 权重的初始值
* 权值衰减:不可设为0，无法进行学习
![](https://s3.bmp.ovh/imgs/2022/05/05/870ebe919ec12c1f.png)
* 权重初始值影响着激活值的分布。例如，使用sigmod函数，接近0和1的导数值很小，造成反向传播过程中梯度值不断减小。
>梯度消失是传统神经网络训练中非常致命的一个问题，其本质是由于链式法则的乘法特性导致的。
* 各层的分布值要有适当的广度。理由如下：
![](https://s3.bmp.ovh/imgs/2022/05/05/ce39573e1b5ccbf0.png)
* Xavier初始值：如果前一层的节点数为n，则初始值使用标准差为$\ \frac{1}{\sqrt{n}}$的分布
  * 目的：为了使各层激活值呈现出具有相同广度的分布
  * 代码：
  ```python
  node_num = 100#前一层节点数
  w = np.random.randn(node_num,node_num) / np.sqrt(node_num)
  ```
* ReLU的权重初始值：He初始值
   * 如果前一层的节点数为n，则初始值使用标准差为$\ \frac{2}{\sqrt{n}}$的分布
## Batch Normlization算法
* 优点：![](https://s3.bmp.ovh/imgs/2022/05/05/5a8e4f7d26e3f001.png)
* 图示：![](https://s3.bmp.ovh/imgs/2022/05/05/df5865f2f9ab70ba.png)
* 实现：
   * 1. 正则化
  ![](https://s3.bmp.ovh/imgs/2022/05/05/c9c70403d9d9e25d.png)
   * 2. 缩放和平移变换![](https://s3.bmp.ovh/imgs/2022/05/05/657c37bb936620b1.png)
## 正则化
* **过拟合：过拟合指的是只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态。机器学习的目标是提高泛化能力，即便是没有包含在训练数据里的未观测数据，也希望模型可以进行正确的识别。**
   * 过拟合主要原因：
     * 模型拥有大量参数，表现力强
     * 训练数据少
* **权值衰减：** 权值衰减是一直以来经常被使用的一种抑制过拟合的方法。该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合。很多过拟合原本就是因为权重参数取值过大才发生的。
  ![](https://s3.bmp.ovh/imgs/2022/05/05/bebb2ae2135c4895.png)
  * 范数：![](https://s3.bmp.ovh/imgs/2022/05/05/464cc4a3d9fa83ec.png)
* **Dropout：通过每次随机删除神经元节点来抑制过拟合**
  ![](https://s3.bmp.ovh/imgs/2022/05/05/50800f5a5a709e19.png)
  其概念图如下：
  ![](https://s3.bmp.ovh/imgs/2022/05/05/83c38444f0c39796.png)
  代码实现：
 ![](https://s3.bmp.ovh/imgs/2022/05/05/55510a3d4a9d9676.png)
 细节说明：
 ![](https://s3.bmp.ovh/imgs/2022/05/05/b1df5e40e50d369b.png)![](https://s3.bmp.ovh/imgs/2022/05/05/283d1a079398d5c1.png)
 ## 超参数的验证与优化
 **验证数据不可用作测试数据**
 方法：![](https://s3.bmp.ovh/imgs/2022/05/05/2d115cf664fc306d.png)
 ## 疑问
 **为什么要强调各激活值分布的广度，其本身不应该是数据特征的一部分吗**
 **为什么Dropout可以随意删除神经元，这不是重要的数据特征吗**

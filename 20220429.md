# 20220429学习小结
1. Affine层：即实现`out = np.dot(self.x,self.W.T)+self.b`。注意计算图的顺序和各矩阵的行列数
   @import "20220429Affine计算图.png"
* 注意：$$ \frac{\alpha L}{\alpha X} $$和$$X$$的维度相同
* 注意图片第三步：加和
* 实现代码：
```python
import numpy as np
#affine层实现
class affine():
    def __init__(self,W,b):
        self.W = W
        self.b = b
        self.x = None
        self.dW = None
        self.db = None
    #forward()
    def forward(self,x):
        self.x = x
        out = np.dot(self.x,self.W.T)+self.b
        return out
    #backward()
    def backward(self,dout):
        dx = np.dot(dout,self.W.T)
        self.dW = np.dot(self.x.T,dout)
        self.db = np.sum(dout,axis = 0)
        return dx
```
2. Softmax-with-Loss层
* 简易版计算图如图：
   @import "20220429Softmax_with_Loss简易计算图.png"
* 实现代码
```python
#softmax_with_loss层的实现
class soft_with_loss():
    def __init__(self):
        self.y = None
        self.t = None
        self.loss = None
    #forward()
    def forward(self,x,t):
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y,t) 
        return self.loss
    #backward()
    def backward(self,dout = 1):
        batch_size = self.t.shape[0]
        dx = (self.y-self.t)/batch_size
        return dx
```
3. 误差反向传播法的实现
* 步骤介绍：
  @import "20220429神经网络学习全貌图.png"
* 参数介绍
  @import "20220429神经网络参数介绍.png"
* 实现代码
```python
class TwoLayerNet():
    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):
        #初始化权重
        self.params = {}
        self.params['W1'] = weight_init_std*np.random.randn(input_size,hidden_size)
        self.params['b1'] = weight_init_std*np.random.randn(hidden_size)
        self.params['W2'] = weight_init_std*np.random.randn(hidden_size,output_size)
        self.params['b2'] = weight_init_std*np.random.randn(output_size)
        #生成层
        self.layers = OrderedDict()
        self.layers['Affine1'] = Affine(self.params['W1'],self.params['b1'])
        self.layers['ReLU1'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W2'],self.params['b2'])

        self.Lastlayer = SoftmaxWithLoss()

    #predict()
    def predict(self,x):
        for layer in self.layers.values:
            x = layer.forward(x)
        return x
    #loss()
    def loss(self,x,t):
        y = self.predict(x)
        return self.Lastlayer.forward(y,t)
    #
    def accuracy(self,x,t):
        y = self.predict(x)
        y = np.argmax(y,axis=1)
        if t.ndim !=1 :
            t = np.argmax(t,axis=1)
        accuracy = np.sum( y==t )/float(x.shape[0])
        return accuracy
    def gradient(self,x,t):
        self.loss(x,t)#前向
        #后向
        dout = 1
        dout = self.Lastlayer.backward(dout)

        layers = list(self.layers.values())
        layers.reverse
        for layer in layers:
            dout = layer.backward(dout)
        #设定
        grads = {}
        grads['W1'] = self.layers['Affine1'].dW
        grads['b1'] = self.layers['Affine1'].db
        grads['W2'] = self.layers['Affine1'].dW
        grads['b2'] = self.layers['Affine1'].db

        return grads
```
* 注意这一步。OrderedDict()是有序字典，可以记录插入顺序
```python
        #生成层
        self.layers = OrderedDict()
        self.layers['Affine1'] = Affine(self.params['W1'],self.params['b1'])
        self.layers['ReLU1'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W2'],self.params['b2'])

        self.Lastlayer = SoftmaxWithLoss()
```
  因此在前向传播时：
```python
    #predict()
    def predict(self,x):
        for layer in self.layers.values:
            x = layer.forward(x)
        return x
```
3. **关于sigmod和Relu激活函数：**
>ReLU相比于Sigmoid几乎是碾压的，如果能用ReLU且能用Sigmoid直接ReLU
>那么Sigmoid存在的意义呢？
对于二元分类问题，输出层的激活函数只能是sigmoid
详细分析见[深度学习基础-ReLU和Sigmoid对比](https://blog.csdn.net/u010848594/article/details/107403174)
### 关于使用自己的numerical_gradient超维度的原因分析
自己所写的按照书中，有函数并没有考虑到多维的情况。